{"remainingRequest":"D:\\强化学习大作业\\代码\\AVS\\node_modules\\@vue\\cli-plugin-babel\\node_modules\\thread-loader\\dist\\cjs.js!D:\\强化学习大作业\\代码\\AVS\\node_modules\\@vue\\cli-plugin-babel\\node_modules\\babel-loader\\lib\\index.js!D:\\强化学习大作业\\代码\\AVS\\node_modules\\@vue\\cli-service\\node_modules\\cache-loader\\dist\\cjs.js??ref--1-0!D:\\强化学习大作业\\代码\\AVS\\node_modules\\@vue\\cli-service\\node_modules\\vue-loader-v16\\dist\\index.js??ref--1-1!D:\\强化学习大作业\\代码\\AVS\\src\\components\\contents\\RLSection.vue?vue&type=script&lang=js","dependencies":[{"path":"D:\\强化学习大作业\\代码\\AVS\\src\\components\\contents\\RLSection.vue","mtime":1659888584526},{"path":"D:\\强化学习大作业\\代码\\AVS\\babel.config.js","mtime":1659357830317},{"path":"D:\\强化学习大作业\\代码\\AVS\\node_modules\\@vue\\cli-plugin-babel\\node_modules\\cache-loader\\dist\\cjs.js","mtime":1659358180332},{"path":"D:\\强化学习大作业\\代码\\AVS\\node_modules\\@vue\\cli-plugin-babel\\node_modules\\thread-loader\\dist\\cjs.js","mtime":1659358180467},{"path":"D:\\强化学习大作业\\代码\\AVS\\node_modules\\@vue\\cli-plugin-babel\\node_modules\\babel-loader\\lib\\index.js","mtime":1659358180188},{"path":"D:\\强化学习大作业\\代码\\AVS\\node_modules\\@vue\\cli-service\\node_modules\\cache-loader\\dist\\cjs.js","mtime":1659358180332},{"path":"D:\\强化学习大作业\\代码\\AVS\\node_modules\\@vue\\cli-service\\node_modules\\vue-loader-v16\\dist\\index.js","mtime":1659358217041}],"contextDependencies":[],"result":[{"type":"Buffer","data":"base64:aW1wb3J0IHsgZGVmaW5lQ29tcG9uZW50IH0gZnJvbSAidnVlIjsKaW1wb3J0IEtleXdvcmRzUmVpbmZvcmNlbWVudCBmcm9tICIuLi9jaGFydHMvS2V5d29yZHNSZWluZm9yY2VtZW50LnZ1ZSI7CmltcG9ydCBLZXl3b3Jkc0RlZXByZWluZm9yY2VtZW50IGZyb20gIi4uL2NoYXJ0cy9LZXl3b3Jkc0RlZXByZWluZm9yY2VtZW50LnZ1ZSI7CmltcG9ydCBQdWJsaWNhdGlvbiBmcm9tICIuLi9jaGFydHMvUHVibGljYXRpb24udnVlIjsKaW1wb3J0IE51bWJlclBhcGVyczEgZnJvbSAiLi4vY2hhcnRzL051bWJlclBhcGVyczEudnVlIjsKZXhwb3J0IGRlZmF1bHQgZGVmaW5lQ29tcG9uZW50KHsKICBuYW1lOiAiUkxTZWN0aW9uIiwKICBjb21wb25lbnRzOiB7CiAgICBLZXl3b3Jkc1JlaW5mb3JjZW1lbnQ6IEtleXdvcmRzUmVpbmZvcmNlbWVudCwKICAgIEtleXdvcmRzRGVlcHJlaW5mb3JjZW1lbnQ6IEtleXdvcmRzRGVlcHJlaW5mb3JjZW1lbnQsCiAgICBQdWJsaWNhdGlvbjogUHVibGljYXRpb24sCiAgICBOdW1iZXJQYXBlcnMxOiBOdW1iZXJQYXBlcnMxCiAgfQp9KTs="},{"version":3,"sources":["D:\\强化学习大作业\\代码\\AVS\\src\\components\\contents\\RLSection.vue"],"names":[],"mappings":"AAgSA,SAAQ,eAAR,QAA8B,KAA9B;AACA,OAAO,qBAAP,MAAkC,qCAAlC;AACA,OAAO,yBAAP,MAAsC,yCAAtC;AACA,OAAO,WAAP,MAAwB,2BAAxB;AACA,OAAO,aAAP,MAA0B,6BAA1B;AAQA,eAAe,eAAe,CAAC;AAC7B,EAAA,IAAI,EAAE,WADuB;AAE7B,EAAA,UAAU,EAAE;AACV,IAAA,qBAAqB,EAArB,qBADU;AAEV,IAAA,yBAAyB,EAAzB,yBAFU;AAGV,IAAA,WAAW,EAAX,WAHU;AAIV,IAAA,aAAY,EAAZ;AAJU;AAFiB,CAAD,CAA9B","sourcesContent":["<template>\r\n  <section id=\"RL-section\" class=\"section is-medium\">\r\n    <h1 class=\"title\">自动驾驶与强化学习</h1>\r\n  </section>\r\n  \r\n  \r\n  <div class=\"content\">\r\n    <keywords-reinforcement/>\r\n    <keywords-deepreinforcement/>\r\n  </div>\r\n  <article class=\"message is-link\">\r\n    <div  class=\"message-body\">\r\n      尽管深度学习技术在自动驾驶中取得了令人鼓舞的成果，但是仍然远远不能实现自动驾驶汽车的完全自主，特别是在决策、运动规划和车辆控制方面。基于深度学习的技术有其固有的缺点，其中的主要缺点是需要数据集来学习预测模型，并且不能自校正累积误差。为了克服这些问题，研究人员正在尝试应用强化学习等其他技术，这些技术可以无需收集数据并且直接从环境中学习。强化学习模型能够通过不断的试错来学习如何执行目标任务。\r\n      <br/>\r\n      <br/>\r\n      在这一部分，我们涵盖了现有的基于强化学习的方法，旨在解决自动驾驶问题，包括：(1) 决策；(2) 运动规划；(3) 车辆控制；(4) 自动驾驶汽车的社会行为。这些是自动驾驶应用的主要领域。\r\n      <br/>\r\n      <br/>\r\n      在讨论强化学习在自动驾驶任务中的应用之前，我们简要回顾一下自动驾驶环境中的状态空间、动作空间和奖励机制。\r\n    </div>\r\n  </article>\r\n\r\n  <div class=\"content\">\r\n    <publication/>\r\n  </div>\r\n\r\n  <article class=\"message is-link\">\r\n    <div id=\"状态空间、动作空间和奖励机制\"  class=\"message-body\">\r\n      <b>5.1 状态空间、动作空间和奖励机制</b>\r\n      <br/>\r\n      <br/>\r\n      为了成功地将强化学习应用于自动驾驶任务，设计适当的状态空间、动作空间和奖励函数非常重要。Leurent等人（2018）对自动驾驶研究中使用的不同状态和动作表示法进行了综合评述。\r\n      <br/>\r\n      <br/>\r\n      自动驾驶汽车通常使用的状态空间特征包括：自身的位置、航向和速度，以及自车载传感器视野范围内的其他障碍物。为了避免状态空间维度的变化，通常会围绕自动驾驶汽车建立笛卡尔坐标系或极坐标系下的占用网格。这种操作进一步增加了车道信息，例如车道号、路径曲率、自身车辆过去和未来的轨迹；纵向信息，例如碰撞时间(TTC)；以及最后的场景信息，例如交通法规和信号位置。\r\n      <br/>\r\n      <br/>\r\n      使用原始传感器数据，如相机图像、激光雷达、雷达等，会为我们提供更好的上下文信息，然而使用压缩的抽象数据大大降低了状态空间的复杂性。介于这两者之间的中等大小的状态空间表示，如2D鸟瞰图（BEV）是无法从传感器获得的，但是它仍然接近场景的真实空间状态。下图是自顶向下视图的图示，显示出了网格、过去和未来的轨迹以及关于场景的语义信息，例如交通信号的位置。这种状态空间表示保留了道路的空间布局，而基于图形的表示则无法保留。\r\n    </div>\r\n  </article>\r\n  <div class='pic'>\r\n      <img id=\"2D鸟瞰图\" src='../../assets/2D.png' width=\"1000\" height=\"1000\">\r\n      <p>图：2D鸟瞰图</p>\r\n  </div>\r\n\r\n  <article class=\"message is-link\">\r\n    <div class=\"message-body\">\r\n      自动驾驶汽车控制策略必须控制许多不同的执行器。用于车辆控制的连续值执行器包括转向角、节气门和制动器。其他执行器，例如齿轮的变化是离散的。为了降低复杂性并允许应用仅适用于离散动作空间（如DQN）的强化学习算法，可将连续值执行器（如转向角、节气门和制动器）的调节范围划分为一系列间隔区间大小相等的离散值，以将动作空间均匀离散化。此外，有人建议在对数空间进行离散化，因为实际驾驶中选择的转向角多数都靠近中心值。然而，离散化也有其自身的缺点，如果动作之间的步长值太大，可能会导致车辆运动轨迹不平稳或不稳定。此外，当选择用于执行器的离散值数量时，我们需要在具有足够的离散值以实现平滑控制和不具有太多离散值以提升控制效率之间权衡。作为离散化的替代方案，我们可以选择直接学习策略的DRL算法来处理连续值执行器（例如DDPG）。时间抽象选项框架（Sutton等人，1999）也可以用来简化选择动作的过程，该框架选择options而不是某个基本动作。这些options代表着一个子策略，它可以将一个基本动作扩展到多个时间步长。\r\n      <br/>\r\n      <br/>\r\n      为应用于自动驾驶技术的强化学习算法设计奖励函数仍然是一个悬而未决的问题。为自动驾驶任务设计的奖励标准包括：朝向目的地行进的距离、自我车辆的速度、使自我车辆保持静止、与其他道路使用者或场景物体的碰撞、人行道上的违规行为、保持在车道上、以及在避免极端加速、制动或转向和遵守交通规则的同时保持舒适和稳定性。\r\n    </div>\r\n  </article>\r\n\r\n  <div class='pic'>\r\n      <img id=\"自动驾驶模拟器\" src='../../assets/zdjsmnq.png' width=\"900\" height=\"1000\">\r\n      <p>表：自动驾驶模拟器</p>\r\n  </div>\r\n\r\n  <article class=\"message is-link\">\r\n    <div id=\"模拟器和场景生成工具\"  class=\"message-body\">\r\n      <b>5.2 模拟器和场景生成工具</b>\r\n      <br/>\r\n      <br/>\r\n      自动驾驶数据集中的训练集包含不同场景的图像和标签对，我们能够利用它来实现监督学习。强化学习算法需要一个特殊环境，其中“状态-动作对”需要是可恢复的，同时需要分别对车辆状态、环境以及环境和主体的运动和动作的进行随机性建模。\r\n      <br/>\r\n      <br/>\r\n      各种模拟器被广泛地用于训练和验证强化学习算法。各种高保真感知模拟器能够模拟照相机、激光雷达和雷达，部分模拟器也能够提供车辆状态信息和运动信息。Rosique等人（2019）为读者提供了自动驾驶领域中使用的传感器和模拟器的完整综述。\r\n      <br/>\r\n      <br/>\r\n      在现实世界中进行昂贵的评估之前，自动驾驶算法所学习的驾驶策略需要先在模拟环境中进行测试。Cutler等人（2014）提出了一种多保真度强化学习（MFRL）框架，该框架让我们能同时使用多个模拟器。在MFRL框架中，一系列保真度越来越高的模拟器被用于表示状态动力学（以及计算成本），这提高了我们训练和验证强化学习算法的能力，同时让我们能够以更少的代价找到与真实世界最为接近的策略。\r\n      <br/>\r\n      <br/>\r\n      CARLA Challenge 是German Ros等人（2019）年推出的一个基于CARLA模拟器的自动驾驶任务竞赛，其中具有国家公路交通安全管理局报告中描述的碰撞前场景。该竞赛系统在关键场景中对自动驾驶算法进行评估，例如：自身车辆失去控制、自身车辆对视野外的障碍物做出反应、变道以避开缓慢的前方车辆等。自动驾驶算法的得分为在不同赛道上行驶的总距离的一个函数，并且违法行为将会扣除部分总得分。\r\n    </div>\r\n  </article>\r\n  \r\n  <div class=\"content\">\r\n    <number-papers1/>\r\n  </div>\r\n\r\n  <article class=\"message is-link\">\r\n    <div id=\"强化决策\"  class=\"message-body\">\r\n      <b>5.3 决策</b>\r\n      <br/>\r\n      <br/>\r\n      通过从经验和环境中学习，强化学习技术目前正在为自动驾驶汽车提供更好和更精确的信息。\r\n      <br/>\r\n      <br/>\r\n      You等人（2018）提出马尔可夫决策过程（MDP）对自动驾驶汽车与周围车辆的相互作用进行建模。MDP允许自动驾驶汽车在超车或尾随另一辆车时，根据道路结构做出适当的决定。不过，他们的方法仍需要在不同的场景中进行更多的测试来验证。\r\n      <br/>\r\n      <br/>\r\n      Hoel等人（2018）提出了一种在高速公路上做出决策（如变道、加速和制动）的方法。他们利用Deep Q Network (DQN)来训练他们的提案并预测正确的决策。不过，他们的方法并不能在其他情况下保证安全性（如环形路和十字路口）。\r\n      <br/>\r\n      <br/>\r\n      无信号交叉口被认为是自动驾驶汽车做出准确且及时决策的最具挑战性的场景之一。Isele等人（2018）提出了一种基于DQN的方法来导航自动驾驶汽车安全的通过交叉口。他们提案的结果优于传统方法（Hafner等人，2013；Alonso等人，2011）。\r\n      <br/>\r\n      <br/>\r\n      Okuyama等人（2018）将卷积神经网络和强化学习相结合，在具有车道标志和静态障碍物(例如人、停止的车辆)的模拟环境中训练自动驾驶汽车。CNN利用由自动驾驶汽车的前置摄像头拍摄的图像，以提取道路的主要状态特征。这些特征被反馈给DQN以预测下一步的行动。不足的是，他们的方法没有考虑动态障碍。\r\n      <br/>\r\n      <br/>\r\n      Hoel等人（2019）提出了一种策略决策方法，该方法结合了蒙特卡罗树搜索（MCTS）和强化学习算法，以在两种高速公路驾驶情况下控制自动驾驶汽车（包括连续高速公路驾驶和退出高速公路）。他们的提案在模拟高速公路环境中进行了测试，初步结果显示了RL和MCTS相结合的有效性。\r\n      <br/>\r\n      <br/>\r\n      Hoel等人（2020）针对三车道的单向高速公路，提出了另一种用于自动驾驶汽车的策略决策方法。他们使用强化学习来估计驾驶行为，例如加速、留在车道上、左转、右转和刹车。结果表明，随着时间的推移，他们的方法学会了做出有效的决策。\r\n      <br/>\r\n      <br/>\r\n      Ye等人（2019）研究了一种强化学习方法，允许自动驾驶汽车在一条直行道路上进行车道变更(无信号灯控制)，该算法已经在模拟环境中利用Q学习进行了训练。然而，他们的方法需要在复杂的场景中进行更多的测试。\r\n      <br/>\r\n      <br/>\r\n      Sun等人（2020）提出了一种基于强化学习的针对重型自动驾驶汽车的决策方法，即深度确定性策略梯度（DDPG）算法。该网络接收自动驾驶汽车的信息状态（包括速度、与其他车辆的距离）然后做出决定。模拟结果显示，他们的方法学会了通过环境快速做出决定。\r\n      <br/>\r\n      <br/>\r\n      Duan等人（2020）提出了一种方法来训练自动驾驶汽车从四周环境中学习并做出三种决策（包括车道变换、右车道变换和左车道变换）。然而，他们只关注高速公路的情况，而没有处理其他情况（如交叉路口和城市交通）。\r\n      <br/>\r\n      <br/>\r\n      Chen等人（2019a）提出了一种在车辆密集的环形交叉口上控制自动驾驶汽车的方法。他们的方法从传感器获取周围每个物体的信息（例如速度和位置）；该方法使用DQN预测正确的动作。他们在Carla模拟器中模拟了他们的算法，模拟结果显示出该方法在解决环形交叉口驾驶情况的乐观前景。\r\n      <br/>\r\n      <br/>\r\n      Wolf等人（2017）使用DQN在模拟环境中训练自动驾驶汽车。利用摄像头，自动驾驶汽车识别道路并采取转弯动作（例如，左、半左、直、半右和右）。然而，他们的建议需要考虑到静态和动态的障碍。\r\n      <br/>\r\n      <br/>\r\n      Likmeta等人（2020）提出了针对三种场景（变道、交叉路口、环形交叉口）的决策方法。他们使用强化学习来训练自动驾驶汽车在不同的情况下做出决策。 \r\n      <br/>\r\n      <br/>\r\n      Huang等人（2019b）提出了一种基于强化学习的方法，通过利用状态信息（例如，车辆速度和道路距离）训练自动驾驶汽车预测所需的决策，以采取动作（加速、减速和转向）。\r\n      <br/>\r\n      <br/>\r\n      Hu等人（2019）提出了一种基于强化学习的方法，在两个场景的融合中控制自动驾驶汽车的决策方法。他们的方案在所有测试场景中实现了零碰撞。然而，这种方法并不能保证安全，特别是在机动性较高的实际交通中。\r\n      <br/>\r\n      <br/>\r\n      Wang等人（2020）提出了一种在复杂高速道路中生成驾驶策略的方法。基于反馈给DQN的传感器信息（例如，车辆位置和速度）来学习如何为复杂高速公路中的每辆车辆做出决策（加速、减速、保持车道、右转和左转）。模拟结果表明，该方法提高了道路的安全性。\r\n      <br/>\r\n      <br/>\r\n      Zhou等人（2021）提出了一种混合交通中自动驾驶车辆协同变道的多智能体强化学习（MARL）框架。其中每一辆自动驾驶汽车根据相邻的自动驾驶汽车和人类驾驶汽车的运动做出车道变换决策。该方法提出了一种多目标奖励函数，以结合燃油效率，驾驶舒适性和自动驾驶的安全性。实验结果表明，该方法在效率、安全性和驾驶员舒适度方面均优于现有常见方法。\r\n    </div>\r\n  </article>\r\n  <div class='pic'>\r\n      <img id=\"强化学习决策流程\" src='../../assets/rljclc.png' width=\"700\" height=\"1000\">\r\n      <p>图：强化学习决策流程</p>\r\n  </div>\r\n\r\n  <article class=\"message is-link\">\r\n    <div id=\"强化运动规划\"  class=\"message-body\">\r\n      <b>5.4 运动规划</b>\r\n      <br/>\r\n      <br/>\r\n      Nosrati等人（2018）提出了一种基于强化学习的分层方法，使用DQN在多车道的道路上控制自动驾驶汽车。该方法的目标是安全地避开障碍物(如其他车辆和摩托车)。这项工作还需要在复杂的场景中进行测试(例如，城市交通和十字路口)。\r\n      <br/>\r\n      <br/>\r\n      Paxton等人（2017）研究了一种基于强化学习的蒙特卡罗树搜索（MCTS）方法，用以生成长期运动规划。该方法主要是让自动驾驶汽车避免在交叉路口发生碰撞。然而，该方法还需要在其他复杂的环境中进行更多的测试。\r\n      <br/>\r\n      <br/>\r\n      Zhu等（2018）模拟了自动驾驶汽车在与环境交互作用下的学习过程。基于来自四个摄像机视图的数据，结果表明强化学习算法能够适应不同的交通状况。\r\n      <br/>\r\n      <br/>\r\n      You等人（2019）对自动驾驶汽车和高速公路交通之间的相互作用进行了建模，并考虑了道路几何结构。基于Q学习，自动驾驶汽车从环境中学习动作（例如，车道变换、速度保持、加速和制动）。仿真结果表明，该方法能够适应多车道多车辆的交通状况。\r\n      <br/>\r\n      <br/>\r\n      Fayjie等人（2018）提出了一种运动规划方法，用于在两种场景下控制自动驾驶汽车（包括城市交通和五车道的高速公路）。该方法使用CNN提取来自相机和激光雷达的数据特征。这些特征被输入给DQN以估计正确的动作（例如，继续前进、向左、向右、加速和刹车）。然而，该方法没有考虑到其他道路参与者。\r\n      <br/>\r\n      <br/>\r\n      Chen等人（2020a）提出了一种可解释的端到端方法来解决复杂城市场景中的自动驾驶问题(交叉口和环形交叉口)。强化学习模型将相机和激光雷达的图像作为输入，并避免密集交通中的碰撞。实验结果表明，该方法优于其他强化学习方法（Mnih等人，2015；Lillicrap等人，2015年；Fujimoto等人，2018；Haarnoja等人，2018)。\r\n      <br/>\r\n      <br/>\r\n      Cao等人（2020）提出了一种基于强化学习的高速公路出口规划方法。该方法基于对环境的观察来估计离开高速公路的预期。结果显示，该方法将离开高速公路的成功率增加了5%到50%。\r\n      <br/>\r\n      <br/>\r\n      Chen等人（2020c）提出了一种基于DQN控制自动驾驶汽车的方法。CNNs和LSTM一起用于从摄像机图像中提取状态信息。这些特征被输入到DQN以学习运动命令，例如直行、右转和在十字路口左转。仿真结果表明，与DDQN方法相比该方法学习速度更快（Wang等，2016）。然而，该方法没有考虑障碍物（例如，行人、骑自行车的人和其他车辆）。\r\n      <br/>\r\n      <br/>\r\n      即使深度学习能够从专业驾驶员的停车数据中学习，人类的知识也不能保证高效停车。为此，Zhang等人（2020a）提出了一种强化学习方法，使用MCTS来训练自动驾驶汽车学习停车策略。该方法实现了高效停车。\r\n      <br/>\r\n      <br/>\r\n      Wang等人（2020a）提出了一种运动规划方法，该方法检测车辆轨迹中的障碍物，并使用强化学习方法自主地避开它们。他们在多车道和不同交通密度的道路上训练了他们的方法。\r\n      <br/>\r\n      <br/>\r\n      Lu等人（2020）利用MDP来实现自动驾驶汽车处理复杂场景（包括没有交通信号的左转和多车道合并）。该方法的目标是训练自动驾驶汽车以实现在这两个场景中成功导航。实验结果证明了该方法的有效性和高效性。\r\n      <br/>\r\n      <br/>\r\n      Kasra等人（2021）提出了一种基于强化学习的方案，用于解决存在不确定性的情况下进行自动驾驶汽车的运动规划。该方案专注于解决传感和感知的不确定性，这是由有限的视野、遮挡和感知范围引起的。该方案应用于两种不同的强化学习算法用于验证，Soft Actor-Critic和DQN。与传统的强化学习算法相比，该方法产生了更好的运动规划行为。\r\n      <br/>\r\n      <br/>\r\n      Tung等人（2022）提出了一个基于学习的规划器DriveIRL，该规划器使用逆强化学习（IRL）在密集的城市交通中驾驶汽车。该规划器生成一组不同的轨迹建议，使用轻量级的且可解释的过滤器筛选这些轨迹，之后使用学习的模型对剩余的每个轨迹进行评分，最佳轨迹由自动驾驶汽车的低级控制器进行跟踪。他们在真实数据集上训练评分轨迹模型，并且在交通繁忙的拉斯维加斯大道进行了自动驾驶展示。\r\n    </div>\r\n  </article>\r\n  <div class='pic'>\r\n      <img id=\"强化学习运动规划方法\" src='../../assets/rlydghff.png' width=\"900\" height=\"1000\">\r\n      <p>表：强化学习运动规划方法</p>\r\n  </div>\r\n  \r\n\r\n  <article class=\"message is-link\">\r\n    <div id=\"强化车辆控制\"  class=\"message-body\">\r\n      <b>5.5 车辆控制</b>\r\n      <br/>\r\n      <br/>\r\n      强化学习的目标是找到一个最佳的控制指令(例如，改变速度，刹车，或加速) ，通过一种迭代的方法探索环境。环境根据自动驾驶汽车当前的行为来奖励并纠正未来的错误。\r\n      <br/>\r\n      <br/>\r\n      Li 等人（2017）提出了一种在多车道高速公路上控制自动驾驶汽车行驶的方法。该方法使用强化学习执行三个控制命令（减速、硬减速和维护）。\r\n      <br/>\r\n      <br/>\r\n      Li 等人（2019a）将感知模块和控制模块相结合。感知模块以道路图像作为CNNs的输入提取状态信息特征。这些特性被输入给DQN来从环境中学习转向控制。该方法在不同的轨道上进行了测试，初步结果显示自动驾驶汽车学会了有效的控制。\r\n      <br/>\r\n      <br/>\r\n      Zhu等人（2020）提出了一种基于深度确定性策略梯度（DDPG，lillicrap 等，2015）控制自动驾驶汽车速度以避免碰撞的强化学习方法。仿真结果表明了该方法在安全性和舒适性方面的有效性。\r\n      <br/>\r\n      <br/>\r\n      Amini 等人（2020）提出了一种强化学习方法，用于在不同的天气条件（晴天和雨天)、光照情况(白天和夜晚)和道路类型(乡村和高速公路)下控制自动驾驶汽车。自动驾驶汽车会观察物体（例如树木、汽车和行人），并且主动绕开它们。仿真结果表明了该方法在实际道路上的自适应能力。\r\n      <br/>\r\n      <br/>\r\n      Guo等人（2020）提出了一种基于强化学习的方法，用于在三车道高速公路中横向控制自动驾驶汽车。该方法的目标是安全地实现变道。结果表明，该方法改善了交通流量和交通容量。\r\n      <br/>\r\n      <br/>\r\n      Wu等人（2020）研究了一种差分变速极限（DVSL）方法，用于在五车道高速公路中控制自动驾驶汽车的速度。DVSL 被建模为一个MDP问题，并且利用SUMO模拟器来训练自动驾驶汽车，使其从与环境的相互作用中学习。试验结果表明，该方法提高了高速公路行驶的安全性。\r\n      <br/>\r\n      <br/>\r\n      Chen等人（2020b）提出了一个使用蒙特卡罗树搜索（MCTS）的强化学习方法，用于控制自动驾驶汽车执行不同的机动以避免碰撞。他们将控制过程建模为MDP问题，并使用MCTS来产生转向角。该方法显示了更高的控制稳定性和更高的避免意外事件的成功率。\r\n      <br/>\r\n      <br/>\r\n      Zhang等人（2018）研究了一种基于DQN和Double-Q学习的自动驾驶汽车速度控制方法。他们用来自真实世界的数据训练该方法，结果显示该方法在价值准确性方面有所改进。 \r\n      <br/>\r\n      <br/>\r\n      Baheri等人（2020）提出了一种在城市驾驶中保持车道的方法。他们的建议是从环境中提取状态观测值，并使用强化学习在CARLA模拟器中训练自动驾驶汽车。他们在两个城镇和不同的天气条件下对自动驾驶汽车进行了模拟，并且成功地完成了车道保持任务。\r\n      <br/>\r\n      <br/>\r\n      Bouton等人（2020）使用强化学习和博弈论解决密集交通中的机动问题。该方法被建模为MDP问题，以实现自动驾驶汽车在合并场景中保持或改变车道。结果表明，与现有方法相比，该方法能够更有效地学习。\r\n      <br/>\r\n      <br/>\r\n      Ye等人（2020）提出了一种高速公路自动换道策略，该策略使用了最近策略优化（PPO）和强化学习。利用车辆和周围车辆的状态，自动驾驶汽车学习如何避免碰撞和实现平稳机动。测试结果表明，该方法可以学习如何有效、安全地执行车道变换操作。\r\n      <br/>\r\n      <br/>\r\n      Toromanoff等人（2020）提出了一种强化学习方法来解决复杂情况（包括车道保持、行人和车辆避让）。他们使用CARLA模拟器来训练模型，只使用一个摄像头来观察周围环境以实现城市驾驶。他们使用DQN来训练自动驾驶汽车从如何处理以前的情况中学习。仿真结果表明，该方法可以推广到未知环境。\r\n      <br/>\r\n      <br/>\r\n      类似地，Jaritz等人（2018）仅使用一个摄像头来观察环境，利用一种强化学习结构（Mnih等人，2016）让自动驾驶汽车学习如何估计控制命令（转向、加速、制动）。他们用不同的道路结构（例如，转弯和山丘）、图形学（例如，季节和位置）和物理学（例如，道路附着力）来处理轨迹。然而，该方法没有考虑碰撞。\r\n      <br/>\r\n      <br/>\r\n      Wang等人（2018）研究了在单向三车道的高速公路上控制自动驾驶汽车进行车道变换的问题。该方法考虑到了周围车辆的行为，并使用Q学习来实现车道变换。实验结果显示了该方法在学习车道变换方面的乐观前景。\r\n      <br/>\r\n      <br/>\r\n      Liang等（2018）提出了一种可控模仿强化学习(CIRL)方法，利用DDPG方法（Lillicrap等人，2015）和CARLA模拟器在复杂场景中（车辆和行人）控制自动驾驶汽车。该方法的结果优于现有的强化学习技术（Dosovitskiy等人，2017）。\r\n      <br/>\r\n      <br/>\r\n      Shi等人（2022）提出了一种基于双边控制模型（BCM）的深度强化学习（DRL）框架，用于实现优于人类驾驶策略的汽车跟随。该方法同时考虑前后车辆，并将双边信息集成到状态和奖励函数中。此外，该方法使用分散的多智能体强化学习框架为每个智能体生成相应的控制动作。仿真结果表面，该方法学习的策略优于人类驾驶策略。\r\n      <br/>\r\n      <br/>\r\n      Edoardo等人（2022）提出了一种基于强化学习的方法，用于控制自动驾驶赛车实现稳健的循迹驾驶。该方法受到上层运动规划器所生成的轨迹的限制。该方法的利用强化学习的启发式特性，同时利用传统规划方法中分层控制结构的可靠性。模拟实验表明，该方法实现了更低的碰撞概率，并且实现了比端到端方法更低的单圈时间。\r\n    </div>\r\n  </article>\r\n  <div class='pic'>\r\n      <img id=\"强化学习车辆控制流程\" src='../../assets/rlclkzlc.png' width=\"700\" height=\"1000\">\r\n      <p>图：强化学习车辆控制流程</p>\r\n  </div>\r\n\r\n  <article class=\"message is-link\">\r\n    <div id=\"强化社会行为\"  class=\"message-body\">\r\n      <b>5.6 车辆社会行为</b>\r\n      <br/>\r\n      <br/>\r\n      任何自动驾驶模型系统都必须能够实时处理城市交通中的道路使用者行为。在这方面，强化学习方法证明了其在理解道路使用者行为并做出正确决策的能力。\r\n      <br/>\r\n      <br/>\r\n      Saleh等人（2018）提出了城市交通环境中的行人意图预测问题。基于对行人过去轨迹的观察，他们的方法使用循环神经网络和强化学习来预测行人的未来行为。实验结果显示，该方法显著改进了预测效果。\r\n      <br/>\r\n      <br/>\r\n      Li等人（2020a）提出了一种基于强化学习的方法，使用MDP识别行人行为并避开行人。该方法考虑了车辆和行人都在同一条结构化双车道道路上的情况。车辆在道路上行驶，而一名行人正在等待过马路，这种情况可能会造成事故，该方法的目标是避免这种碰撞。\r\n      <br/>\r\n      <br/>\r\n      Li等人（2020b）提出了一种在复杂场景下估计未来行人位置的强化学习方法。该方法的目标是训练自动驾驶汽车学会识别道路上每个行人的未来姿态。该方法的结果在预测精度方面优于现有的方法（Van den Berg等人，2008；Chen等，2017b；Everett等人，2018；Gupta等人，2018）。\r\n      <br/>\r\n      <br/>\r\n      Nasernejad等人（2021）将交叉口行人和自动驾驶汽车之间的相互作用建模为MDP问题。该方法能够让自动驾驶汽车根据行人在环境中的行为来避开行人。然而，该方法仅限于行人和车辆的相互作用，而其他相互作用(例如自行车)也应该被考虑。\r\n      <br/>\r\n      <br/>\r\n      Wang等人（2021）提出了一种基于协商感知运动规划的强化学习框架。该框架采用强化学习来调整运动规划器的驾驶风格，通过动态修改运动规划器的预测视野长度，自适应地应对复杂环境中的不同事件。该框架将自动驾驶汽车与其他交通参与者之间的交互建模为马尔可夫决策过程。他们将该方法应用在模拟和现实世界中的狭窄车道导航，结果现实该方法优于常见的其他方案。\r\n      <br/>\r\n      <br/>\r\n      Britod等人（2022）提出了一种在密集交通场景中的交互感知模型。该方法通过深度强化学习模型来学习一种交互感知策略，并用于推理道路上其他车辆的驾驶意图，进一步指导自身车辆的动态规划。该方法在两种模拟环境中（高速公路合并和无保护的左转弯）进行了实验。结果表明，该方法减少了碰撞次数，并提高了成功率。\r\n    </div>\r\n  </article>\r\n  \r\n\r\n</template>\r\n\r\n\r\n<script>\r\nimport {defineComponent} from \"vue\";\r\nimport KeywordsReinforcement from \"../charts/KeywordsReinforcement.vue\";\r\nimport KeywordsDeepreinforcement from \"../charts/KeywordsDeepreinforcement.vue\";\r\nimport Publication from \"../charts/Publication.vue\";\r\nimport NumberPapers1 from \"../charts/NumberPapers1.vue\";\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nexport default defineComponent({\r\n  name: \"RLSection\",\r\n  components: {\r\n    KeywordsReinforcement,\r\n    KeywordsDeepreinforcement,\r\n    Publication,\r\n    NumberPapers1\r\n\r\n  },\r\n});\r\n</script>\r\n\r\n<style lang=\"scss\">\r\n.card {\r\n  width: 70%;\r\n  margin: 0 auto;\r\n  margin-bottom: 80px;\r\n}\r\n\r\n.pic {\r\n  margin-bottom: 60px;\r\n  margin-top: 0;\r\n}\r\n\r\n.custom {\r\n  border-top: 10px dotted steelblue;\r\n  margin-top: 80px;\r\n}\r\n\r\n#RL-section {\r\n  background-image: url(\"../../assets/bj.jpg\");\r\n\r\n}\r\n\r\nli {\r\n  letter-spacing: 0.1em;\r\n  font-size: 20px;\r\n}\r\n\r\n</style>\r\n\r\n"],"sourceRoot":""}]}